{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K #Idk why backend is required.\n",
    "import numpy as np\n",
    "import random\n",
    "import queue\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to store memory\n",
    "class RingBuf:\n",
    "    def __init__(self, size):\n",
    "        # Pro-tip: when implementing a ring buffer, always allocate one extra element,\n",
    "        # this way, self.start == self.end always means the buffer is EMPTY, whereas\n",
    "        # if you allocate exactly the right number of elements, it could also mean\n",
    "        # the buffer is full. This greatly simplifies the rest of the code.\n",
    "        self.data = [None] * (size + 1)\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        \n",
    "    def append(self, element):\n",
    "        self.data[self.end] = element\n",
    "        self.end = (self.end + 1) % len(self.data)\n",
    "        # end == start and yet we just added one element. This means the buffer has one\n",
    "        # too many element. Remove the first element by incrementing start.\n",
    "        if self.end == self.start:\n",
    "            self.start = (self.start + 1) % len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[(self.start + idx) % len(self.data)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.end < self.start:\n",
    "            return self.end + len(self.data) - self.start\n",
    "        else:\n",
    "            return self.end - self.start\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.data)):\n",
    "            yield self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that chooses the best action:\n",
    "def choose_best_action(model,state):\n",
    "    x = np.expand_dims(state, axis=0)\n",
    "    mask=np.ones((1,6))\n",
    "    #mask = [1 , 1, 1 , 1 , 1, 1]\n",
    "    print(mask.shape)\n",
    "    best_action = np.argmax(model.predict([x, mask], verbose=1))\n",
    "    print(best_action)\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified \n",
    "def one_hot_encoding(value):\n",
    "    output = np.zeros((1,6))\n",
    "    output[0][value] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The keras model:\n",
    "#modified\n",
    "def Q_model(n_actions,ATARI_SHAPE):\n",
    "    # I think n_actions is the one hot coded action array. Wrong. It is the number of actions.\n",
    "    # Input takes the one hot coded action array as a mask.\n",
    "    # We assume a theano backend here, so the \"channels\" are first.\n",
    "    #ATARI_SHAPE = (1, 84, 84, 4) #not sure about this when tensorflow is used\n",
    "\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = keras.layers.Input((ATARI_SHAPE), name='frames')\n",
    "    actions_input = keras.layers.Input((n_actions,), name='mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    # normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
    "    \n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = keras.layers.Conv2D(16, (8, 8), strides=(4, 4), activation='relu',input_shape=(ATARI_SHAPE,))(frames_input)\n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = keras.layers.Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv_1)\n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = keras.layers.core.Flatten()(conv_2)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = keras.layers.Dense(n_actions)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    #filtered_output = keras.layers.merge([output, actions_input], mode='mul')\n",
    "    filtered_output = keras.layers.multiply(([output, actions_input]))\n",
    "\n",
    "    model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "    model.compile(optimizer, loss='mse')\n",
    "    return model  #Idk if this return statement is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function to get epsilon\n",
    "def get_epsilon_for_iteration(iteration):\n",
    "    if(iteration>1000000):\n",
    "        return 0.1\n",
    "    else:\n",
    "        return(1 - 9*iteration/10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay():\n",
    "    batch = 32 #number of elements in a batch.\n",
    "    batch_state_mem = RingBuf(batch)\n",
    "    batch_action_mem = RingBuf(batch)\n",
    "    batch_q_mem = RingBuf(batch)\n",
    "    rand_int_list = []\n",
    "    for i in range(batch):\n",
    "        index = gen_rand()\n",
    "        while index in rand_int_list:\n",
    "            index = gen_rand()\n",
    "        rand_int_list.append(index)\n",
    "        batch_state_mem.append(state_memory._getitem_(index))\n",
    "        batch_action_mem.append(action_memory._getitem_(index))\n",
    "        Qcalc = reward_memory._getitem_(index) + gamma*(np.max(model.predict([next_state_memory._getitem_(index), np.ones(n_actions)], verbose=1)))\n",
    "        Q_calc = Qcalc * action_memory._getitem_(index)\n",
    "        batch_q_mem.append(Q_calc)\n",
    "    model.fit(x=[batch_state_mem, batch_action_mem], y = batch_q_mem, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    return resize(rgb2gray(img), (110, 84))[18:115 - 13, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to transform the reward.\n",
    "def transform_reward(reward):\n",
    "    #return np.sign(reward)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to fit a batch\n",
    "#As for now, this function is a waste.\n",
    "def fit_batch(model, gamma, start_states, actions, rewards, next_states, is_terminal):\n",
    "    \"\"\"Do one deep Q learning iteration.\n",
    "    \n",
    "    Params:\n",
    "    - model: The DQN\n",
    "    - gamma: Discount factor (should be 0.99)\n",
    "    - start_states: numpy array of starting states\n",
    "    - actions: numpy array of one-hot encoded actions corresponding to the start states\n",
    "    - rewards: numpy array of rewards corresponding to the start states and actions\n",
    "    - next_states: numpy array of the resulting states corresponding to the start states and actions\n",
    "    - is_terminal: numpy boolean array of whether the resulting state is terminal\n",
    "    \n",
    "    \"\"\"\n",
    "    # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "    next_Q_values = model.predict([next_states, np.ones(actions.shape)])\n",
    "    # The Q values of the terminal states is 0 by definition, so override them\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
    "    # Fit the keras model. Note how we are passing the actions as the mask and multiplying\n",
    "    # the targets by the actions.\n",
    "    model.fit([start_states, actions], actions * Q_values[:, None], nb_epoch=1, batch_size=len(start_states), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate random numbers\n",
    "def gen_rand():\n",
    "    return random.randint(Q_calc_memory.start, Q_calc_memory.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"mu...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps = 1015\n",
      "Number of steps = 1191\n",
      "Number of steps = 1337\n",
      "Number of steps = 1220\n",
      "Number of steps = 1275\n",
      "Number of steps = 1035\n",
      "Number of steps = 1106\n",
      "Number of steps = 1156\n",
      "(1, 6)\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1\n",
      "returned\n",
      "(1, 6)\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1\n",
      "returned\n",
      "(1, 6)\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "1\n",
      "returned\n",
      "Number of steps = 1110\n",
      "Number of steps = 1089\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1001,) (1,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-56b612344951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mQcalc_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mQ_calc_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQcalc_memory\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_calc_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mno_replays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1001,) (1,6) "
     ]
    }
   ],
   "source": [
    "#The initial function which does q-iteration\n",
    "# Create the environment\n",
    "env = gym.make('Pong-v0')\n",
    "no_memory_elements=1000\n",
    "n_actions = env.action_space.n\n",
    "state_memory = RingBuf(no_memory_elements)\n",
    "next_state_memory = RingBuf(no_memory_elements)\n",
    "#Q_calc_memory = RingBuf(no_memory_elements)\n",
    "action_memory = RingBuf(no_memory_elements)\n",
    "reward_memory = RingBuf(no_memory_elements)\n",
    "frame=env.reset()\n",
    "state=preprocess(frame)\n",
    "next_state=preprocess(frame)\n",
    "\n",
    "for i in range (3):\n",
    "    action=env.action_space.sample()\n",
    "    new_frame, reward, is_done, _ = env.step(action)\n",
    "    new_frame= preprocess(new_frame)\n",
    "    state=np.dstack((state,new_frame))\n",
    "\n",
    "ATARI_SHAPE=state.shape\n",
    "\n",
    "no_episodes = 10\n",
    "iteration=0\n",
    "gamma=0.99 #discount factor\n",
    "alpha = 0.01 #learning rate\n",
    "model = Q_model(n_actions, ATARI_SHAPE)\n",
    "for i in range(no_episodes):\n",
    "    # Reset it, returns the starting frame\n",
    "    frame=env.reset()\n",
    "    state=preprocess(frame);\n",
    "    next_state=preprocess(frame);\n",
    "    # Render\n",
    "    #env.render()\n",
    "    for i in range (3):\n",
    "        action=env.action_space.sample()\n",
    "        new_frame, reward, is_done, _ = env.step(action)\n",
    "        new_frame= preprocess(new_frame)\n",
    "        state=np.dstack((state,new_frame))\n",
    "        next_state=np.dstack((next_state,new_frame))\n",
    "        action1=action\n",
    "    is_done = False\n",
    "    count=3\n",
    "    #state_mem_episode = np.copy(state)\n",
    "    #next_state_mem_episode = np.copy(next_state)\n",
    "    while not is_done:\n",
    "        if(count%4 ==0):\n",
    "            count=count+1\n",
    "            iteration=iteration+1\n",
    "            epsilon = get_epsilon_for_iteration(iteration)\n",
    "            # Choose the action \n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = choose_best_action(model, state)\n",
    "                print(\"returned\")\n",
    "            new_frame, reward, is_done, _ = env.step(action)\n",
    "            if(is_done):\n",
    "                #continue #problem here : it goes out of for loop also?\n",
    "                pass\n",
    "            new_frame= preprocess(new_frame)\n",
    "            reward=transform_reward(reward)\n",
    "            next_state=np.delete(next_state,0,2)\n",
    "            next_state=np.dstack((next_state,new_frame))\n",
    "            ##Qcalc = reward + gamma* (np.max(model.predict([next_state, np.ones(n_actions)], verbose=0)))\n",
    "            ##Q_calc = Qcalc * one_hot_encoding(action)\n",
    "            ##Q_calc_memory.append(Q_calc)\n",
    "            state_memory.append(state)\n",
    "            next_state_memory.append(next_state)\n",
    "            reward_memory.append(reward)\n",
    "            action_memory.append(one_hot_encoding(action))\n",
    "            state=np.delete(state,0,2)\n",
    "            state=np.dstack((state,new_frame))\n",
    "            action1=action\n",
    "        else:\n",
    "            new_frame, reward, is_done, _ = env.step(action1)\n",
    "            new_frame= preprocess(new_frame)\n",
    "            state=np.delete(state,0,2)\n",
    "            state=np.dstack((state,new_frame))\n",
    "            next_state=np.delete(next_state,0,2)\n",
    "            next_state=np.dstack((next_state,new_frame))\n",
    "            count=count+1\n",
    "    print(\"Number of steps = {}\".format(count))\n",
    "\n",
    "Qcalc_memory = reward_memory.data + gamma*(np.max(model.predict([next_state_memory.data, np.ones((1,6))], verbose=0, steps = 1)))\n",
    "Q_calc_memory = Qcalc_memory * one_hot_encoding(action)\n",
    "model.fit(x=[state_memory.data, action_memory.data], y = Q_calc_memory, verbose = 1)\n",
    "no_replays = 2\n",
    "for j in range (no_replays):\n",
    "    experience_replay()\n",
    "final()\n",
    "env.close()\n",
    "#env.close()\n",
    "#fit batches now.\n",
    "#also fit in each inner loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The final function which plays\n",
    "# Reset the environment, returning the starting frame\n",
    "def final()\n",
    "    new_frame = env.reset()\n",
    "    state.extend(new_frame)\n",
    "    # Render\n",
    "    env.render()\n",
    "    for i in range (3):\n",
    "            action=env.action_space.sample()\n",
    "            new_frame, reward, is_done, _ = env.step(action)\n",
    "            new_frame= preprocess(new_frame)\n",
    "            state.extend(new_frame)\n",
    "    count=0\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        action = choose_best_action(model, state)\n",
    "        # Perform the best action, returns the new frame, reward and whether the game is over\n",
    "        frame, reward, is_done, _ = env.step(action)\n",
    "        new_frame= preprocess(new_frame)\n",
    "        del state[0:shape_to_del]\n",
    "        state.extend(new_frame)    \n",
    "        # Render\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
